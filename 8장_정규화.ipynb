{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3p2kKooiCmphv91IuHxSj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qqq3964/NLP-/blob/main/8%EC%9E%A5_%EC%A0%95%EA%B7%9C%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정규화를 위한 다양한 방법들\n",
        "1. MinMax scaler\n",
        "2. RobustScaler\n",
        "3. StandardScaler\n",
        "\n",
        "아래는 diabetes 데이터를 이용한 이진분류 예측 딥러닝이다.</br>\n",
        "이를 위해 캐글에서 데이터셋을 가져왔고 뉴럴 네트워크로 구성해보았다."
      ],
      "metadata": {
        "id": "cCGR1--Kzqry"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hCCSwlgzoGh",
        "outputId": "7cfeba69-e12e-4c96-8ec9-ec90d29be1fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  archive.zip\n",
            "  inflating: diabetes.csv            \n"
          ]
        }
      ],
      "source": [
        "!unzip archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# cuda 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "gEsACxeD0Myi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlrYS9MC0nUE",
        "outputId": "ba99a2d2-d95b-4ffa-fd38-f7daa9f88c2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/diabetes.csv')\n",
        "X = df[df.columns[:-1]]\n",
        "y = df['Outcome']\n",
        "\n",
        "# X는 값을 values를 이용해서 모두 바꿔주었고 y는 tensor로 바꿔줌\n",
        "X = X.values\n",
        "y = torch.tensor(y.values)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33)"
      ],
      "metadata": {
        "id": "cuSt11G10uGo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ms = MinMaxScaler()\n",
        "ss = StandardScaler()\n",
        "\n",
        "X_train = ss.fit_transform(X_train)\n",
        "X_test = ss.fit_transform(X_test)\n",
        "# y_train ,y_test값을 (?,1) 형태로 만들어준다.\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "y_train = ms.fit_transform(y_train)\n",
        "y_test = ms.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "qiFX-Pq01o6a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkBX0r1U8hYy",
        "outputId": "47454056-b9b5-4a0f-9504-412264a42b11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(254, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class customDataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.len = len(self.X)\n",
        "  # index 번호를 통해서 객체 자체로 접근한다.\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "metadata": {
        "id": "DaPpX78419KZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = customDataset(torch.FloatTensor(X_train),\n",
        "                           torch.FloatTensor(y_train))\n",
        "test_data = customDataset(torch.FloatTensor(X_test),\n",
        "                           torch.FloatTensor(y_test))\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=64,shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=64,shuffle=False)"
      ],
      "metadata": {
        "id": "ZUEvsAOG3rFX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이런식으로 맞춰줌"
      ],
      "metadata": {
        "id": "67V7S0iD9wn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.FloatTensor(X_train)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIGIoila8Hoi",
        "outputId": "b08a6afd-69f8-4940-a4ba-084a347c54be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0185, -0.0125, -0.0507, -1.2757, -0.6773, -0.2947,  0.7320,  0.0371])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.FloatTensor(y_train)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKhvnWbe9jjG",
        "outputId": "3b38cfa5-bdb4-4ec4-97b7-9e4a9d577ecf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 설계"
      ],
      "metadata": {
        "id": "5lvUkH0mB_ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class binaryClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(binaryClassifier,self).__init__()\n",
        "    # 칼럼이 8개이므로 처음에 들어오는 input은 8임\n",
        "    self.layer1 = nn.Linear(8,64,bias=True)\n",
        "    self.layer2 = nn.Linear(64,64,bias=True)\n",
        "    self.layer_out = nn.Linear(64,1,bias=True)\n",
        "    self.relu = nn.ReLU() # activation 함수\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "    # 정규화를 통해 빠른 연산과 gradient vanishing 현상에 대해 줄 일 수 있다.\n",
        "    self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "    self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    x1 = self.relu(self.layer1(inputs))\n",
        "    x1 = self.batchnorm1(x1)\n",
        "    x2 = self.relu(self.layer2(x1))\n",
        "    x2 = self.batchnorm2(x2)\n",
        "    x2 = self.dropout(x2)\n",
        "    out = self.relu(self.layer_out(x2))\n",
        "    return out"
      ],
      "metadata": {
        "id": "n4-GPe5T9sCd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습"
      ],
      "metadata": {
        "id": "pJOt-DCLCt8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000+1\n",
        "print_epoch = 100\n",
        "LEARNING_RATE = 1e-2\n",
        "\n",
        "model = binaryClassifier()\n",
        "# cuda 연산을위해 model과 data를 gpu 메모리 위로 올려줘야한다.\n",
        "model.to(device)\n",
        "print(model)\n",
        "# binary cross entropy -> loss 함수 2개의 이진분류이니 Logits 만약 다중분류면 softmax사용\n",
        "BCE = nn.BCEWithLogitsLoss()\n",
        "# optimizer 선언 \n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=LEARNING_RATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DMJOhurB-gM",
        "outputId": "5e1eb52b-be2b-4683-dea7-eaf572ef91ff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binaryClassifier(\n",
            "  (layer1): Linear(in_features=8, out_features=64, bias=True)\n",
            "  (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 성능 측정 함수 정의\n",
        "\n",
        "def accuracy(y_pred,y_test):\n",
        "  # 반올림함수로 torch.round를 정의\n",
        "  y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "  correct_results_sum = (y_pred_tag == y_test).sum().float() # 실제 정답과 모델의 결과가 일치하는 개수를 실수 형태로 변수에 저장\n",
        "  acc = correct_results_sum/y_test.shape[0]\n",
        "  acc = torch.round(acc*100)\n",
        "  return acc                          "
      ],
      "metadata": {
        "id": "aedUtk6XCRbc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습"
      ],
      "metadata": {
        "id": "PMrNZCosD_2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  iteration_loss = 0. # 변수를 0으로 초기화\n",
        "  iteration_accuracy = 0.\n",
        "\n",
        "  # 모델학습\n",
        "  model.train()\n",
        "  for i,data in enumerate(train_loader):\n",
        "    X,y = data\n",
        "    # 모델에 넣어줄때 gpu memory 위로 같이 올려줘야한다.\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # 이제 모델에 넣고 예측\n",
        "    y_pred = model(X)\n",
        "    loss = BCE(y_pred,y.float())\n",
        "\n",
        "    iteration_loss += loss # loss 값을 변수에 누적하여 저장\n",
        "    iteration_accuracy += accuracy(y_pred,y.float()) # 모델 정확도 누적 저장\n",
        "    \n",
        "    optimizer.zero_grad() # 초기화\n",
        "    loss.backward() # 데이터셋 배치사이즈로 돌았으니 backpropagation을 실행해줘야한다.\n",
        "    optimizer.step()\n",
        "\n",
        "  if(epoch % print_epoch == 0):\n",
        "    # len(train_loader)로 나누는 이유는 전체 학습후에 그러면 한 iterlation 을 돌았기 때문 그래서 저런식으로 나눠준것이다. iteration 횟수만큼 누적되어 loss가 나오기떄문\n",
        "    print('Train: epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch,iteration_loss/len(train_loader),iteration_accuracy/len(train_loader)))\n",
        "  \n",
        "  iteration_loss = 0. # 변수를 0으로 초기화\n",
        "  iteration_accuracy = 0.\n",
        "  \n",
        "  # 모델 평가\n",
        "  model.eval()\n",
        "  for i,data in enumerate(test_loader):\n",
        "    X,y = data\n",
        "    # 모델에 넣어줄때 gpu memory 위로 같이 올려줘야한다.\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # 이제 모델에 넣고 예측\n",
        "    y_pred = model(X)\n",
        "    loss = BCE(y_pred,y.float())\n",
        "\n",
        "    iteration_loss += loss # loss 값을 변수에 누적하여 저장\n",
        "    iteration_accuracy += accuracy(y_pred,y.float()) # 모델 정확도 누적 저장\n",
        "\n",
        "  if(epoch % print_epoch == 0):\n",
        "    print('Test: epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch,iteration_loss/len(test_loader),iteration_accuracy/len(test_loader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Veq0h32D3ts",
        "outputId": "17469560-0f16-4861-a7b0-4626e6ff6e00"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch: 0 - loss: 0.77103; acc: 37.556\n",
            "Test: epoch: 0 - loss: 0.72878; acc: 26.500\n",
            "Train: epoch: 100 - loss: 0.62363; acc: 80.667\n",
            "Test: epoch: 100 - loss: 0.71492; acc: 72.500\n",
            "Train: epoch: 200 - loss: 0.62884; acc: 71.222\n",
            "Test: epoch: 200 - loss: 0.70390; acc: 75.750\n",
            "Train: epoch: 300 - loss: 0.60356; acc: 81.333\n",
            "Test: epoch: 300 - loss: 0.73325; acc: 74.750\n",
            "Train: epoch: 400 - loss: 0.61287; acc: 82.333\n",
            "Test: epoch: 400 - loss: 0.73294; acc: 74.250\n",
            "Train: epoch: 500 - loss: 0.67980; acc: 70.889\n",
            "Test: epoch: 500 - loss: 0.70963; acc: 76.750\n",
            "Train: epoch: 600 - loss: 0.62406; acc: 77.222\n",
            "Test: epoch: 600 - loss: 0.76122; acc: 76.250\n",
            "Train: epoch: 700 - loss: 0.63093; acc: 72.000\n",
            "Test: epoch: 700 - loss: 0.74302; acc: 74.750\n",
            "Train: epoch: 800 - loss: 0.62209; acc: 81.889\n",
            "Test: epoch: 800 - loss: 0.74624; acc: 77.500\n",
            "Train: epoch: 900 - loss: 0.59166; acc: 83.667\n",
            "Test: epoch: 900 - loss: 0.76293; acc: 76.500\n",
            "Train: epoch: 1000 - loss: 0.60047; acc: 83.111\n",
            "Test: epoch: 1000 - loss: 0.80926; acc: 76.250\n"
          ]
        }
      ]
    }
  ]
}